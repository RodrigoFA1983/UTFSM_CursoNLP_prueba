{"cells":[{"cell_type":"code","execution_count":null,"id":"0ad10d7a","metadata":{"id":"0ad10d7a","colab":{"base_uri":"https://localhost:8080/","height":777},"executionInfo":{"status":"ok","timestamp":1643155538952,"user_tz":180,"elapsed":16422,"user":{"displayName":"Pía Maldonado R.","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjkTRsPAz3Q0IOLkF8tjG_gLqKj2aFLdYYfwc5bPKE=s64","userId":"04925426408852941917"}},"outputId":"4b2b94db-a560-4f7d-fd8a-86a959af5b14"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pdfplumber\n","  Downloading pdfplumber-0.6.0.tar.gz (46 kB)\n","\u001b[K     |████████████████████████████████| 46 kB 2.7 MB/s \n","\u001b[?25hCollecting pdfminer.six==20211012\n","  Downloading pdfminer.six-20211012-py3-none-any.whl (5.6 MB)\n","\u001b[K     |████████████████████████████████| 5.6 MB 5.5 MB/s \n","\u001b[?25hCollecting Pillow>=8.4\n","  Downloading Pillow-9.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n","\u001b[K     |████████████████████████████████| 4.3 MB 49.3 MB/s \n","\u001b[?25hCollecting Wand>=0.6.7\n","  Downloading Wand-0.6.7-py2.py3-none-any.whl (139 kB)\n","\u001b[K     |████████████████████████████████| 139 kB 60.3 MB/s \n","\u001b[?25hRequirement already satisfied: chardet in /usr/local/lib/python3.7/dist-packages (from pdfminer.six==20211012->pdfplumber) (3.0.4)\n","Collecting cryptography\n","  Downloading cryptography-36.0.1-cp36-abi3-manylinux_2_24_x86_64.whl (3.6 MB)\n","\u001b[K     |████████████████████████████████| 3.6 MB 47.0 MB/s \n","\u001b[?25hRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography->pdfminer.six==20211012->pdfplumber) (1.15.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography->pdfminer.six==20211012->pdfplumber) (2.21)\n","Building wheels for collected packages: pdfplumber\n","  Building wheel for pdfplumber (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pdfplumber: filename=pdfplumber-0.6.0-py3-none-any.whl size=33688 sha256=2c2bef8da1798862188ddb39bb14c691e1baffd30864c585e81ee5fa1fbe87e5\n","  Stored in directory: /root/.cache/pip/wheels/58/56/fe/2e93d842ffa9ea97746c1ab253d43502ed61c0689361a0224e\n","Successfully built pdfplumber\n","Installing collected packages: cryptography, Wand, Pillow, pdfminer.six, pdfplumber\n","  Attempting uninstall: Pillow\n","    Found existing installation: Pillow 7.1.2\n","    Uninstalling Pillow-7.1.2:\n","      Successfully uninstalled Pillow-7.1.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n","Successfully installed Pillow-9.0.0 Wand-0.6.7 cryptography-36.0.1 pdfminer.six-20211012 pdfplumber-0.6.0\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["PIL"]}}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Collecting unidecode\n","  Downloading Unidecode-1.3.2-py3-none-any.whl (235 kB)\n","\u001b[?25l\r\u001b[K     |█▍                              | 10 kB 23.8 MB/s eta 0:00:01\r\u001b[K     |██▉                             | 20 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 30 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 40 kB 3.6 MB/s eta 0:00:01\r\u001b[K     |███████                         | 51 kB 3.6 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 61 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 71 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 81 kB 3.3 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 92 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 102 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 112 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 122 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 133 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 143 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 153 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 163 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 174 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 184 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 194 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 204 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 215 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 225 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 235 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 235 kB 4.1 MB/s \n","\u001b[?25hInstalling collected packages: unidecode\n","Successfully installed unidecode-1.3.2\n"]}],"source":["import pandas as pd\n","import os\n","import re\n","import numpy as np\n","from numpy import log \n","from math import sqrt \n","\n","!pip install pdfplumber\n","!pip install unidecode\n","\n","#Libreria para importar texto de pdf\n","import pdfplumber\n","\n","#librerias para texto\n","import nltk #Natural Language Tool Kit\n","#importamos tokenizador de texto\n","from nltk.tokenize import word_tokenize \n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","#stemmer mediate algoritmo de porter\n","from nltk.stem import PorterStemmer \n","\n","#stemmer de libreria snowball\n","from nltk import SnowballStemmer \n","import string #funciones adicionales para cadenas\n","\n","\n","\n","#para reemplazo de caracteres especiales latinos\n","import unidecode \n","import unicodedata\n","\n","#para visualizacion\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","%matplotlib inline "]},{"cell_type":"code","execution_count":null,"id":"fa8cefdc","metadata":{"id":"fa8cefdc","colab":{"base_uri":"https://localhost:8080/","height":187},"executionInfo":{"status":"error","timestamp":1643156126868,"user_tz":180,"elapsed":432,"user":{"displayName":"Pía Maldonado R.","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjkTRsPAz3Q0IOLkF8tjG_gLqKj2aFLdYYfwc5bPKE=s64","userId":"04925426408852941917"}},"outputId":"d4517f5f-871a-45bd-a75d-d0c06b13eb22"},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-04db5c7706d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Establecemos directorio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/00-data/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/00-data/'"]}],"source":["#Establecemos directorio\n","dir=os.chdir('/content/drive/MyDrive/00-data/')\n"]},{"cell_type":"code","execution_count":null,"id":"e40be0c8","metadata":{"id":"e40be0c8"},"outputs":[],"source":["#1- CARGAMOS EL CONTENIDO DEL PDF EN UNA LISTA"]},{"cell_type":"code","execution_count":null,"id":"351fa022","metadata":{"id":"351fa022"},"outputs":[],"source":["#Cargamos pdf en un objeto pdfplumber\n","pdf = pdfplumber.open('19900521.pdf')\n","pdf"]},{"cell_type":"code","execution_count":null,"id":"374877b4","metadata":{"id":"374877b4"},"outputs":[],"source":["# Proceso de carga de pdf\n","page = pdf.pages[0]\n","text = page.extract_text()\n","print(text)"]},{"cell_type":"code","execution_count":null,"id":"3a99aeed","metadata":{"id":"3a99aeed"},"outputs":[],"source":["# Total de paginas del texto\n","total_pages = len(pdf.pages)\n","total_pages"]},{"cell_type":"code","execution_count":null,"id":"9172c0a9","metadata":{"id":"9172c0a9"},"outputs":[],"source":["#indexamos contenido separado por paginas\n","\n","#creamos lista que almacenará el contenido de todo el archivo, segmentado por paginas\n","lista_archivo1=[]\n","\n","#ciclo para cada pagina en el rango del numero de paginas del archivo\n","for pagina in range(total_pages):\n","    #indexamos contenido de texto de cada pagina en una lista auxiliar\n","    lista_pagina=pdf.pages[pagina].extract_text()\n","    \n","    #almacenamos el contenido de texto de la pagina indexada en la lista final del archivo\n","    lista_archivo1.append(lista_pagina)\n","\n","#revisamos contenido de las primeras 5 paginas del documento   \n","for pagina in range(0,4):\n","    print('Página ',str(pagina+1),':\\n',lista_archivo1[pagina], '\\n')\n","    "]},{"cell_type":"code","execution_count":null,"id":"84f5b87c","metadata":{"id":"84f5b87c"},"outputs":[],"source":["#2- VACIAMOS CONTENIDO DEL PDF EN UN DATAFRAME"]},{"cell_type":"code","execution_count":null,"id":"e3d911d2","metadata":{"id":"e3d911d2"},"outputs":[],"source":["#Pasamos la lista de páginas a un dataframe\n","df_paginas=pd.DataFrame(lista_archivo1, columns=['contenido_pagina_crudo'])\n","df_paginas.head(10)"]},{"cell_type":"code","execution_count":null,"id":"c85e7677","metadata":{"id":"c85e7677"},"outputs":[],"source":["#3- LIMPE Y NORMALICE EL CONTENIDO MEDIANTE FUNCION AD HOC"]},{"cell_type":"code","execution_count":null,"id":"683d4127","metadata":{"id":"683d4127"},"outputs":[],"source":["#importamos stopwords\n","from nltk.corpus import stopwords \n","\n","#descargamos stopwords\n","nltk.download('stopwords')\n","\n","#importamos tokenizador de texto\n","from nltk.tokenize import word_tokenize \n","\n","#para reemplazo de caracteres especiales latinos\n","import unidecode \n","import unicodedata\n","\n","#stemmer mediate algoritmo de porter\n","from nltk.stem import PorterStemmer \n","\n","#stemmer de libreria snowball\n","from nltk import SnowballStemmer \n","\n","#creamos lista de stopwords en castellano\n","stopwords=stopwords.words('spanish')\n","\n","#normalizamos stopwords, removemos tildes y otros caracteres latinos para el match con texto normalizado\n","#creamos lista vacía que almacenara palabras normalizadas\n","stop_words=[]\n","#ciclo en el que cada palabra\n","for word in stopwords:\n","    #es normalizada\n","    word_norm = unicodedata.normalize('NFD', word).encode('ascii', 'ignore').decode(\"utf-8\")\n","    \n","    #y almacenada en la nueva lista\n","    stop_words.append(word_norm)\n","    print(word, word_norm)\n","    "]},{"cell_type":"code","execution_count":null,"id":"31930244","metadata":{"id":"31930244"},"outputs":[],"source":["####LEMATIZACIÓN\n","\n","#Con la lematización pasa algo similar, cortar palabras puede hacernos perder su riqueza.\n","snowball = SnowballStemmer(language='spanish')\n","#instanciamos el lematizador de porter\n","#porter= PorterStemmer(languaje='spanish')\n","\n","\n","####NORMALIZACIÓN DE TEXTO\n","\n","def limpieza(texto):\n","    #pasamos a minusculas\n","    texto_norm=texto.lower()\n","    \n","    #removemos espacios al final y al inicio de cada cadena\n","    texto_norm=texto_norm.strip()\n","    \n","    #removemos dobles espacios: ojo que si los eliminamos, concatenamos palabras\n","    texto_norm=re.sub('\\s',' ', texto_norm)\n","    \n","    #nuevo_texto = unidecode.unidecode(nuevo_texto)\n","    texto_norm = unicodedata.normalize('NFD', texto_norm).encode('ascii', 'ignore').decode(\"utf-8\")\n","    \n","    #retenemos caracteres alfanumericos: ojo que incluimos la ñ\n","    texto_norm=re.sub('[^A-Za-zñ]+', ' ', texto_norm)\n","    \n","    #tokenizamos texto: convertir en una lista una cadena\n","    texto_norm = word_tokenize(texto_norm)\n","    \n","    # Eliminación de terminos de largo 1\n","    texto_norm = [token for token in texto_norm if len(texto_norm) > 1]\n","    \n","    #removemos stopwords\n","    texto_norm=[word for word in texto_norm if not word in stop_words]\n","    \n","    #lematizamos texto con el stemmer de snowbal\n","    #TAREA OPCIONAL: pruebe el stemmer de porter y compare resultados\n","    texto_norm=[snowball.stem(word) for word in texto_norm]\n","    \n","    #resultado de la funcion\n","    return(texto_norm)"]},{"cell_type":"code","execution_count":null,"id":"da4f36f8","metadata":{"id":"da4f36f8"},"outputs":[],"source":["#IMPLEMENTACIÓN: la funcion de limpieza y normalizacion\n","df_paginas['contenido_limpio'] = df_paginas['contenido_pagina_crudo'].apply(lambda x: limpieza(x))\n","\n","#revisamos cabecera\n","df_paginas.tail()"]},{"cell_type":"code","execution_count":null,"id":"bf632b44","metadata":{"id":"bf632b44"},"outputs":[],"source":["#creamos columna que identifica cada hoja, se tienen en total las 81 hojas del pdf original\n","df_paginas['pagina']=df_paginas.index+1\n","\n","#revisamos cabecera\n","df_paginas.tail()"]},{"cell_type":"code","source":[""],"metadata":{"id":"2RXcKwFvSII1"},"id":"2RXcKwFvSII1","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"f215af44","metadata":{"id":"f215af44"},"outputs":[],"source":["#4- GENERACION DE GRAFICOS PARA LEY DE ZIPF Y HEAPS"]},{"cell_type":"code","execution_count":null,"id":"2b99bd1f","metadata":{"id":"2b99bd1f"},"outputs":[],"source":["#creamos data frame que intersecta a cada página con cada termino\n","doc_term=df_paginas.explode(column= 'contenido_limpio')\n","\n","#cambiamos nombre de columna para identificar termino\n","doc_term=doc_term.rename(columns={'contenido_limpio':'termino'})\n","\n","#vista de cabecera\n","doc_term.tail(50)"]},{"cell_type":"code","execution_count":null,"id":"0a320473","metadata":{"id":"0a320473"},"outputs":[],"source":["#Construimos el vocabulario\n","#agregamos datos\n","vocabulario=doc_term.groupby(['termino']).count().sort_values(by='pagina', ascending=False).reset_index()\n","\n","#cambiamos nombre para identificar frecuencia del termino\n","vocabulario  = vocabulario.rename(columns={'pagina':'frecuencia'})\n","\n","#agregamos columna con el ranking\n","vocabulario['ranking']=vocabulario.index+1\n","\n","#generamos la frecuencia estimada segun formula de la ley de zipf\n","vocabulario['frecuencia_estimada']=vocabulario.frecuencia[0]/(vocabulario.ranking**2)\n","\n","#revisamos cabecera\n","vocabulario.head(60)"]},{"cell_type":"code","execution_count":null,"id":"187199fd","metadata":{"id":"187199fd"},"outputs":[],"source":["#Graficamos la Ley de Zipf\n","fig, ax = plt.subplots(figsize=(8, 6))\n","ax.plot(vocabulario.ranking, vocabulario.frecuencia_estimada);\n","ax.plot(vocabulario.ranking, vocabulario.frecuencia);\n","\n","# Agregamos titulo y nombres en los ejes\n","plt.title('Sesión del Congreso Pleno - Ley de Zipf')\n","plt.xlabel(\"i\")\n","plt.ylabel(\"f(i)\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"54a2308c","metadata":{"id":"54a2308c"},"outputs":[],"source":["#agregamos datos a nivel de articulos para ley de heaps\n","#tener en cuenta que vamos caracterizar a cada articulo segun el numero de palabras totales\n","#y segun el numero de palabras distintas\n","#doc_term.groupby(['termino']).count().sort_values(by='articulo_num', ascending=False).reset_index()\n","total=pd.DataFrame(doc_term.groupby(['pagina'])['termino'].count())\n","total=total.rename(columns={'termino':'terminos_totales'})\n","\n","distint=pd.DataFrame(doc_term.groupby(['pagina'])['termino'].nunique())\n","distint=distint.rename(columns={'termino':'terminos_distintos'})\n","vocabulario_art=pd.concat([total,distint], axis=1).sort_values(by='terminos_distintos', ascending=True).reset_index()\n","\n","#parametros para la ley de Heaps\n","K=10\n","beta=0.5\n","vocabulario_art['V']=K*vocabulario_art.terminos_totales**(beta)\n","vocabulario_art=vocabulario_art.sort_values(by='V')\n"]},{"cell_type":"code","execution_count":null,"id":"6c955075","metadata":{"id":"6c955075"},"outputs":[],"source":["vocabulario_art"]},{"cell_type":"code","execution_count":null,"id":"6db57c1e","metadata":{"scrolled":true,"id":"6db57c1e"},"outputs":[],"source":["#Graficamos la Ley de Heaps\n","fig, ax = plt.subplots(figsize=(8, 6))\n","ax.plot(vocabulario_art.terminos_totales, vocabulario_art.V);\n","ax.plot(vocabulario_art.terminos_totales, vocabulario_art.terminos_distintos);\n","# Agregamos titulo y nombres en los ejes\n","plt.title('Sesión del Congreso Pleno - Ley de Heaps')\n","plt.xlabel(\"n\")\n","plt.ylabel(\"V\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"09c43c2f","metadata":{"id":"09c43c2f"},"outputs":[],"source":["#REPRESENTACION DE TEXTO TOKENIZADO EN MATRIZ TF-IDF"]},{"cell_type":"code","execution_count":null,"id":"ac9bdf97","metadata":{"id":"ac9bdf97"},"outputs":[],"source":["#instanciamos modelo para vectorizar:\n","def identity_tokenizer(text):\n","    return text\n","\n","tfidf_vect = TfidfVectorizer(tokenizer=identity_tokenizer,\n","                             lowercase=False,\n","                            use_idf=True,\n","                            ngram_range=(1,2))\n","\n","#ajustamos vectorizador a los datos\n","tfidf_vect.fit(df_paginas.contenido_limpio)\n","\n","#implementamos matriz tf-idf\n","tfidf_data = tfidf_vect.transform(df_paginas.contenido_limpio)"]},{"cell_type":"code","execution_count":null,"id":"da76bf87","metadata":{"id":"da76bf87"},"outputs":[],"source":["tfidf_vect.get_feature_names()"]},{"cell_type":"code","execution_count":null,"id":"99731e87","metadata":{"id":"99731e87"},"outputs":[],"source":["tfidf_df=pd.DataFrame(tfidf_data.toarray(), columns=tfidf_vect.get_feature_names())\n","tfidf_df.shape"]},{"cell_type":"code","execution_count":null,"id":"2b5f95c3","metadata":{"id":"2b5f95c3"},"outputs":[],"source":["tfidf_df"]},{"cell_type":"code","execution_count":null,"id":"15efae0a","metadata":{"id":"15efae0a"},"outputs":[],"source":[""]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"colab":{"name":"DataScience - Modulo 9 - Tarea P1.ipynb","provenance":[]}},"nbformat":4,"nbformat_minor":5}